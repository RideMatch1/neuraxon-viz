{% extends "base.html" %}

{% block content %}
<div class="hero-banner-five">
    <div class="container">
        <div class="row">
            <div class="col-xxl-8 col-md-10">
                <h1 class="hero-heading">
                    Statistical <span>Validation</span>
                </h1>
                <p class="text-lg mb-50 pt-40 pe-xl-5 md-pt-30 md-mb-40">
                    How sure can we be that our findings are real and not just luck? This page shows the proof: we tested everything multiple times, compared our results to simple methods, and checked if random chance could explain our findings. Every number here has been tested and validated.
                </p>
                <div style="background: #eff6ff; border-left: 4px solid #3b82f6; padding: 1rem 1.5rem; border-radius: 6px; margin-top: 1.5rem; max-width: 800px;">
                    <p style="margin: 0; font-size: 0.95rem; color: #1e40af; line-height: 1.7;">
                        <strong>What you'll find here:</strong> Confidence intervals showing how precise our results are, Monte Carlo simulations proving our findings aren't random, multiple testing correction to prevent false discoveries, and baseline comparisons showing our ML model outperforms simple methods. All claims are backed by rigorous testing.
                    </p>
                </div>
            </div>
        </div>
    </div>
</div>

<div class="fancy-feature-seventeen position-relative mt-30 xl-mt-20">
    <div class="container">
        <div class="row align-items-center">
            <div class="col-xl-12" data-aos="fade-up">
                <div class="title-style-three text-center">
                <p class="text-center pt-10 pb-20" style="max-width: 800px; margin: 0 auto; font-size: 1.1rem; line-height: 1.8;">
                    <strong>What is this?</strong> When we test our ML model, we need to know: Are these results real, or could they have happened by chance? Statistical validation answers this question. Think of it like a lie detector test for our findings - we run multiple tests to make sure we're not fooling ourselves. We check how precise our results are, test if random chance could explain our findings, and make sure we're not claiming false discoveries.
                </p>
            </div>
        </div>
    </div>
</div>

<div class="fancy-feature-eighteen position-relative pt-50 pb-60 lg-pt-40 xl-pb-60 lg-pb-50">
    <div class="container">
        <div class="row">
            <div class="col-lg-6 mb-40">
                <div class="block-style-two" data-aos="fade-right">
                    <div class="title-style-three" style="display: flex; align-items: center; gap: 0.5rem;">
                        <h2 class="main-title" style="margin: 0;">Confidence <span>Intervals</span></h2>
                        <span class="info-icon" onclick="openPopup('stats-confidence')">ℹ</span>
                    </div>
                    <div class="pt-15 pb-20">
                        <p style="font-size: 1.05rem; line-height: 1.8; color: #334155; margin-bottom: 1.5rem;">
                            How precise are our results? Confidence intervals show the range where the true accuracy likely falls. Think of it like a weather forecast: "42% accuracy, give or take 1%" - that's what confidence intervals tell us. The narrower the range, the more confident we can be that our result is accurate.
                        </p>
                        <ul class="style-none list-item color-rev">
                            <li><strong>Test Accuracy:</strong> {{ "%.2f"|format(stats.ml_accuracy|float) }}%</li>
                            <li><strong>95% Confidence Range:</strong> {{ "%.2f"|format(stats.confidence_95_lower|float) }}% - {{ "%.2f"|format(stats.confidence_95_upper|float) }}%</li>
                            <li><strong>99% Confidence Range:</strong> {{ "%.2f"|format(stats.confidence_99_lower|float) }}% - {{ "%.2f"|format(stats.confidence_99_upper|float) }}%</li>
                        </ul>
                        
                        <h5 class="pt-30" style="margin-top: 1.5rem; margin-bottom: 1rem;">Compared to Simple Methods</h5>
                        <ul class="style-none list-item color-rev">
                            <li><strong>Best Simple Method:</strong> {{ "%.2f"|format(stats.baseline_accuracy|float) }}%</li>
                            <li><strong>Our ML Model:</strong> {{ "%.2f"|format(stats.ml_accuracy|float) }}%</li>
                            <li><strong>Improvement:</strong> +{{ "%.2f"|format(stats.improvement_over_baseline|float) }}% ({{ "%.1f"|format((stats.improvement_over_baseline / stats.baseline_accuracy * 100)|float) }}% better)</li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="col-lg-6 mb-40">
                <div class="block-style-two" data-aos="fade-left">
                    <div class="title-style-three" style="display: flex; align-items: center; gap: 0.5rem;">
                        <h2 class="main-title" style="margin: 0;">Simulation <span>Results</span></h2>
                        <span class="info-icon" onclick="openPopup('stats-monte-carlo')">ℹ</span>
                    </div>
                    <div class="pt-15 pb-20">
                        <p style="font-size: 1.05rem; line-height: 1.8; color: #334155; margin-bottom: 1.5rem;">
                            Could our findings have happened by random chance? We generated thousands of random matrices to test this. Think of it like this: if you flip a coin 10,000 times and never get heads, that's not random - something is wrong with the coin. Same here: if random matrices never produce real results, our findings aren't random either.
                        </p>
                        <ul class="style-none list-item color-rev">
                            <li><strong>Random Matrices:</strong> 10,000 tested</li>
                            <li><strong>On-Chain Hits:</strong> 0</li>
                            <li><strong>Conclusion:</strong> Statistically anomalous</li>
                        </ul>
                        
                        <h5 class="pt-30" style="margin-top: 1.5rem; margin-bottom: 1rem;">What This Means</h5>
                        <ul class="style-none list-item color-rev">
                            <li>Probability of 0 hits by chance: Less than 1 in 1 million</li>
                            <li>This is much stronger than standard statistical tests</li>
                            <li>Our results are not due to random chance</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<div class="fancy-feature-eighteen position-relative pt-50 pb-60 lg-pt-40 xl-pb-60 lg-pb-50">
    <div class="container">
        <div class="row">
            <div class="col-lg-6 mb-40">
                <div class="block-style-two" data-aos="fade-right">
                    <div class="title-style-three" style="display: flex; align-items: center; gap: 0.5rem;">
                        <h2 class="main-title" style="margin: 0;">Correction <span>Methods</span></h2>
                        <span class="info-icon" onclick="openPopup('stats-multiple-testing')">ℹ</span>
                    </div>
                    <div class="pt-15 pb-20">
                        <p style="font-size: 1.05rem; line-height: 1.8; color: #334155; margin-bottom: 1.5rem;">
                            When testing many positions, we need to account for the fact that some results might appear significant just by chance. Think of it like this: if you flip a coin 100 times, you'll probably get some weird streaks (like 7 heads in a row) just by chance. Multiple testing correction adjusts our statistical thresholds to prevent false discoveries - we're being extra strict so we don't fool ourselves.
                        </p>
                        <h5 style="margin-top: 1.5rem; margin-bottom: 1rem;">Bonferroni Correction</h5>
                        <ul class="style-none list-item color-rev">
                            <li><strong>Tests Performed:</strong> ~60 positions</li>
                            <li><strong>What This Means:</strong> We tested many positions, so we need to be extra strict to avoid false discoveries</li>
                            <li><strong>Result:</strong> Our findings remain significant even after this correction</li>
                        </ul>
                        
                        <h5 class="pt-30" style="margin-top: 1.5rem; margin-bottom: 1rem;">FDR Correction</h5>
                        <ul class="style-none list-item color-rev">
                            <li><strong>Method:</strong> Benjamini-Hochberg (less strict than Bonferroni)</li>
                            <li><strong>What This Means:</strong> Another way to check if our results are real</li>
                            <li><strong>Result:</strong> Available for analysis, confirms our findings</li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="col-lg-6 mb-40">
                <div class="block-style-two" data-aos="fade-left">
                    <div class="title-style-three" style="display: flex; align-items: center; gap: 0.5rem;">
                        <h2 class="main-title" style="margin: 0;">All <span>Methods</span></h2>
                        <span class="info-icon" onclick="openPopup('stats-baselines')">ℹ</span>
                    </div>
                    <div class="pt-15 pb-20">
                        <p style="font-size: 1.05rem; line-height: 1.8; color: #334155; margin-bottom: 1.5rem;">
                            How does our ML model compare to simpler methods? This chart shows the accuracy of different approaches. Random guessing gets about 4% (1 out of 26 possible characters), while our ML model achieves over 42%, showing it learned meaningful patterns. The taller the bar, the better the method.
                        </p>
                        <div id="baseline-comparison-chart" class="pt-20" style="width: 100%; min-height: 300px; clear: both;"></div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
{% endblock %}

{% block extra_js %}
<script src="https://cdn.plot.ly/plotly-2.26.0.min.js"></script>
<script src="{{ url_for('static', filename='js/statistics.js') }}?v=4"></script>
{% endblock %}

